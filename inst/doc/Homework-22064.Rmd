---
title: "Homework-22064"
author: "By 22064"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework of 22064}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Directory {#directory}

1. Homework0 [Jump to the Answer](#question1ans)

2. Homework1 [Jump to the Answer](#question2ans)

3. Homework2 [Jump to the Answer](#question3ans)

4. Homework3 [Jump to the Answer](#question4ans)

5. Homework4 [Jump to the Answer](#question5ans)

6. Homework5 [Jump to the Answer](#question6ans)

7. Homework6 [Jump to the Answer](#question7ans)

8. Homework7 [Jump to the Answer](#question8ans)

9. Homework8 [Jump to the Answer](#question9ans)

10. Homework9 [Jump to the Answer](#question10ans)

11. Homework10 [Jump to the Answer](#question11ans)














# Homework0 {#question1ans}

## Question

Go through “R for Beginners” if you are not familiar with R programming.

## Answer

I've read this book and learned a lot about it, but I still need more practice. I will continue to use it as a reference book in my future study.

## Question

Use knitr to produce at least 3 examples (texts, figures, tables).

## Answer

##### 1. Use knitr to produce texts

In addition to the most common text, there are several other forms of text shown as follows.

(a) **Bold**
(b) *Italic*
(c) ~~Strikethrough~~
(d) <u>Underscore</u>
(e) <big>Large font</big>
(f) <small>Small font</small>

 We can use these to optimize our homework.

##### 2. Use knitr to produce tables

Make a summary of the data set of mtcars.

```{r}
summary(mtcars)
```

'mpg' means Miles/(US) gallon,cyl means Number of cylinders. Generated a table of the first seven columns of the data set of mtcars

```{r}
knitr::kable(mtcars[,1:7])
```


##### 3. Use knitr to produce figures

We can conclude that the six-cylinder model has a more even mileage per gallon of gasoline than the other two models.Compared to the six-cylinder and eight-cylinder models, the four-cylinder model has the widest number of miles per gallon of gasoline.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
boxplot(mpg ~ cyl,data = mtcars,
        main="Car Mileage Data",
        xlab = "Number of cylinders",
        ylab = "Miles/(US) gallon")
```

[Back to the Directory](#directory)



















# Homework1 {#question2ans}

## Question 
Based on the Pareto(a,b) distribution, use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution and graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison.

## Answer 
This question uses the inverse transform method to simulate a random sample from the distribution$F(x)=1-(\frac{b}{x})^a,\quad x\ge b>0,a>0$.
We have


$$
\begin{equation}
\begin{split}
\small (\frac{b}{a})^a &= 1-F(x)\\
\Rightarrow \small \frac{b}{x}&=[1-F(x)]^{\frac{1}{a}}\\
\Rightarrow x &= b[1-F(x)]^{-\frac{1}{a}}
\end{split}
\end{equation}
$$

Hence, the probability inverse transformation is $F_{X}^{-1}(u)=b[1-u]^{-\frac{1}{a}}$.
Generate all $n$ required random uniform numbers as vector $u$ and take $a=2,b=2$. Then $2[1-u]^{-\frac{1}{2}}$ is a vector of length $n$ containing the sample $x_1,...,x_n$. 


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
n <- 100
set.seed(4)
u <- runif(n)
x <- 2*(1-u)^{-1/2} # F(x) = 1-(2/x)^2, x>=2


for(i in 1:n){
  while(x[i]<2)
  {
    kkk <- runif(1)
    x[i] <- 2*(1-kkk)^{-1/2} 
  }
} # 确保x中的值均大于2



hist(x, prob = TRUE, main = expression(f(x)==8/x^3))
y <- seq(2, 1000, .01)
lines(y, 8/y^3)
```


**FIGURE**: Probability density histogram of a random sample generated by the inverse transform method in question 1, with the theoretical density $f(x) = \frac{8}{x^3}$ superimposed.

The histogram and density plot in figure above suggests that the empirical and theoretical distributions approximately agree.




## Question
Write a function to generate a random sample of size n from the Beta(a, b) distribution by the acceptance-rejection method and take Beta(3,2) for an example to generate a random sample of size 1000. Finally, graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.


## Answer
This question uses the acceptance-rejection method to generate a random sample of the beta distribution.
Take the Beta function as
$$B(p,q)=\frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}$$
Then take the beta distribution as
$$Beta(x;a,b)=\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)},\quad 0<x<1$$

The PDF of beta distribution can be bell-shaped curves, strictly increasing curves, decreasing curves or even straight lines. When you change the value of $\alpha$ or $\beta$, the shape of the PDF curve changes. The following are examples.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}

num <- 10000
alpha1 <- 8
belta1 <- 2

alpha2 <- 5
belta2 <- 5

alpha3 <- 2
belta3 <- 8

x <- seq(0,1,0.01)

Probability <- dbeta(x,shape1=alpha1,shape2=belta1)

plot(x,Probability,type='l',col='red')
lines(x,dbeta(x,shape1=alpha2,shape2=belta2),col='forestgreen')
lines(x,dbeta(x,shape1=alpha3,shape2=belta3),col='blue3')


```

**FIGURE**: The PDF of beta distribution with (a,b)=(8,2), (a,b)=(5,5) and (a,b)=(2,8).

The figure above shows that if $a + b$ is large enough and $a$  and $b$ are equal, the PDF of beta distribution is approximately normal.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}

num <- 10000
alpha1 <- 2
belta1 <- 1

alpha2 <- 1
belta2 <- 1

alpha3 <- 1
belta3 <- 2

x <- seq(0,1,0.01)

Probability <- dbeta(x,shape1=alpha1,shape2=belta1)

plot(x,Probability,type='l',col='red')
lines(x,dbeta(x,shape1=alpha2,shape2=belta2),col='forestgreen')
lines(x,dbeta(x,shape1=alpha3,shape2=belta3),col='blue3')


```

**FIGURE**: The PDF of beta distribution with (a,b)=(2,1), (a,b)=(1,1) and (a,b)=(1,2).

The beta distribution can be an increasing line or decreasing line, and when a=b=1 the PDF is a horizontal line.


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}

num <- 10000
alpha1 <- 0.8
belta1 <- 0.2

alpha2 <- 0.5
belta2 <- 0.5

alpha3 <- 0.2
belta3 <- 0.8

x <- seq(0,1,0.01)

Probability <- dbeta(x,shape1=alpha1,shape2=belta1)

plot(x,Probability,type='l',col='red')
lines(x,dbeta(x,shape1=alpha2,shape2=belta2),col='forestgreen')
lines(x,dbeta(x,shape1=alpha3,shape2=belta3),col='blue3')


```

**FIGURE**: The PDF of beta distribution with (a,b)=(0.8,0.2), (a,b)=(0.5,0.5) and (a,b)=(0.2,0.8).

When $a<1$  and $b<1$, the PDF of Beta distribution is a U-shaped curve.

It's not easy to find a function to generate a random sample of size n from the Beta(a, b) distribution by the acceptance-rejection method with all possible a and b. There are some relevant discussions, maybe we can get inspired from here: https://stats.stackexchange.com/questions/80934/accept-reject-for-beta-distribution.

Here we take a>1 and b>1 as an example to write such function. When a>1 and b>1, it's easy to know the beta distribution reach the maximum at $x_0=\frac{a-1}{a+b-2}$ and $B_0=Beta(x_0;a,b)=\frac{(\frac{a-1}{a+b-2})^{a-1}(\frac{b-1}{a+b-2})^{b-1}}{B(a,b)}$. Then we can let $g(x)$ be the Uniform(0,1) density and take 
$$c=\begin{cases} [Beta(\theta_0;a,b)]& {[B_0]=B_0}\\ [Beta(\theta_0;a,b)]+1& {[B_0]<B_0} \end{cases}$$
to generate the $B(p,q)$, where [x] indicates the largest integer not greater than x.
When $a=3,b=2$,the $Beta(3,2)$ density is $f(x) = 60 x^2 (1 − x), 0 < x < 1$. Let $g(x)$ be the Uniform(0,1) density. Then $f(x)/g(x) ≤ 12$ for all $0 < x < 1$, so c = 12 . A random x from g(x) is accepted if
$$\frac{f(x)}{cg(x)}=\frac{12x^2(1-x)}{12(1)}=x^2(1-x)>u$$
In the following simulation, we use the counter $j$ for iterations to record how many iterations were actually needed to generate the 1000 beta variates.

```{r}
n <- 1000
j <- 0
k <- 0
set.seed(1234)
y <- numeric(n)
while (k < n) {
  u <- runif(1)
  j <- j + 1
  x <- runif(1) #random variate from g(.)
  if (x^2 * (1-x) > u) {
    #we accept x
    k <- k + 1
    y[k] <- x 
  } 
}
j # the number of experiments for generating n random numbers
```
In this simulation, `r j` iterations were required to generate the 1000 beta variates. Compare the empirical and theoretical percentiles.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
hist(y, prob = TRUE, main = expression(f(x)==12*x^2*(1-x)))
yy <- seq(0, 10, .01)
lines(yy, 12*yy^2*(1-yy))
```


**FIGURE**: Probability density histogram of a random sample generated by the acceptance-rejection method in question 2, with the theoretical density $f(x) = 12x^2(1-x)$ superimposed.


```{r}
#compare empirical and theoretical percentiles
p <- seq(.1, .9, .1)
Qhat <- quantile(y, p) #quantiles of sample
Q <- qbeta(p, 3, 2) #theoretical quantiles
se <- sqrt(p^2 * (1-p) / (n * dbeta(Q, 2, 2))) #see Ch. 1


round(rbind(Qhat, Q, se), 3)
```

The sample percentiles (first line) approximately match the Beta(3,2) percentiles computed by qbeta (second line). Actually, larger numbers of replicates are required for estimation of percentiles where the density is close to zero.

## Question

Simulate a continuous Exponential-Gamma mixture. Suppose that the rate parameter $\Lambda$ has Gamma(r, β) distribution and Y has Exp(Λ) distribution, i.e. $Y|\Lambda ∼ f_Y(y|\lambda)=\lambda e^{-\lambda y}$. Then generate 1000 random observations from this mixture with r = 4 and β = 2.

## Answer
This is an example of a continuous mixture. Suppose $Y|\Lambda ∼ Exp(\Lambda)$, and $\Lambda ∼ Gamma(r, \beta)$. Then generate random observations from this mixture.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}

n <- 1000
r <- 4 
beta <- 2
lambda <- rgamma(n, r, beta) # lambda is random

#now supply the sample of lambda’s as the Poisson mean
x <- rexp(n, lambda) # the mixture

Xx <- x[1:25]

head(matrix(Xx,5,5))

```

Parts of the random observations are shown as above.


## Question

Actually the mixture in Exercise 3.12 has a Pareto distribution with CDF.

$$
F(y)=1-(\frac{\beta}{\beta+y})^{r}, \quad y\geq0
$$

Based on that, generate 1000 random observations from the mixture with r = 4 and β = 2 and compare the empirical and theoretical (Pareto) distributions by graphing the curve.


## Answer

This question illustrates a method of sampling from a Exp-Gamma mixture and compares the sample with the Pareto distribution. 
Suppose $Y|\Lambda ∼ Exp(\Lambda)$, and $\Lambda ∼ Gamma(r, \beta)$. Then the marginal distribution of $Y$ is


$$
\begin{equation}
\begin{split}
& \int_{0}^{\infty}\frac{\beta^r\Lambda^{r-1}e^{-\beta\Lambda}}{\Gamma(r)}\Lambda e^{-\Lambda y}d\Lambda\\
& \stackrel{t=(\beta+y)\Lambda}{\Rightarrow} \int_{0}^{\infty}\frac{\beta^r}{\Gamma(r)(\beta+y)^{r+1}}t^{r}e^{-t}dt\\
&=\frac{\Gamma(r+1) \beta^r}{\Gamma(r)(\beta+y)^{r+1}}
\end{split}
\end{equation}
$$

Hence the distribution function is $F(y)=1-\frac{\Gamma(r+1) }{\Gamma(r)r}(\frac{\beta}{\beta+y})^r$. If $r, \beta$ are integers, $F(y)=1-(\frac{\beta}{\beta+y})^r$.
When $r=4,\beta=2$, the marginal distribution of $Y$ is $\frac{64}{(y+2)^{5}}$, the distribution function is $1-(\frac{2}{2+y})^4$ and the probability inverse transformation is $F_{X}^{-1}(u)=\frac{2}{(1-u)^{1/4}}-2$.
We can from the description of question 3.12 that the Pareto distribution is a mixture of $Exp(\Lambda)$ distributions, where $\Lambda$ has a gamma distribution. Specifically, if $(X|\Lambda = \lambda) ∼ Exp(\lambda)$ and $\Lambda ∼ Gamma(r, \beta)$, then $X$ has the Pareto distribution with CDF
$F(y)=1-(\frac{\beta}{\beta+y})^r, \quad y \geq 0$


```{r}
n <- 1000
r <- 4 
beta <- 2
lambda <- rgamma(n, r, beta) # lambda is random

#now supply the sample of lambda’s as the Poisson mean
x <- rexp(n, lambda) # the mixture


u <- runif(n)
# the Pareto
y <- 2/(1-u)^{1/4}-2 # F(x) = 1-(2/(2+y))^4, x>=-2

```


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
hist(x, prob = TRUE, main = expression(f(x)==64/(2+y)^5))
k <- seq(0, 1000, .01)
lines(k, 64/(2+k)^5)
```

**FIGURE**: Probability density histogram of a random sample of a continuous Exponential-Gamma mixture, with the theoretical density $f(x) = \frac{64}{(y+2)^{5}}$ superimposed.

The histogram and density plot in the figure above suggests that the Exponential-Gamma mixture and the Pareto distributions generated by the inverse transform method approximately agree.

[Back to the Directory](#directory)










# Homework2 {#question3ans}

## Question

This question requires us apply the fast algorithm to randomly permuted numbers of 1,...,n in different size n. And in each n, calculate computation time averaged over 100 simulations, denoted by $a_n$. Then regress $a_n$ on $t_n := n log(n)$, and show the scatter plot and regression line.

## Answer

the flowchart as follow:

1. Specify n from N, the size of numbers to sort and specify x, the replicated experimental units;
2. Generate x out-of-order samples of size N and record them in $Testsample$ matrix;
3. Use $quick_sort$ function to calculate the computation time spent sorting each sample and record them in $Times$ matrix;
4. Take five different n and record all the computation time in the $TTimes$ matrix; (*$TTimes$ is a matrix of 100 $\times$ 5, each column of which is the computation time of 100 simulations in with a specific n*)
5. Calculate the column mean of the matrix $TTimes$ as computation time averaged over 100 simulations and record them in $A=	\left\{ a_n \right\}$;
6. Calculate $t_n=n\text{log}(n)$ and record them in $T$;
7. Regress $A$ on $T$, and show the scatter plot and regression line.

```{r}
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}#??????
}


N <- c(1e4,2e4,4e4,6e4,8e4) 

x <- 100

TTimes <- matrix (0,x,5)

for (k in 1:5){
  n <- N[k]
  Testsample <- matrix(0,n,x)
  Times <- matrix(0,x)
  for (i in 1:x){
    Testsample[1:n,i] <- sample(n)
    Times[i] <- system.time(quick_sort(Testsample[1:n,i]))[1]
  }
  TTimes[1:x,k] <- Times
}


```


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
T <- matrix(0,5)

for  (i in 1:5){
  T[i] <- N[i]*log10(N[i])
}


A <- colMeans(TTimes)

Lm <- lm(A~T)

lie1 <- matrix(A)
lie2 <- matrix(T)
data <- matrix(0,5,2)
data[1:5,1] <- lie1
data[1:5,2] <- lie2
data <- data.frame(data)

LM <- lm(A~T)

library(ggplot2)
ggplot(data, aes(X2, X1))+
  geom_point(size=6,shape=21)+
  geom_smooth(method = "lm")+
  labs(x="nlog(n)",y="an")



```

<center> **FIGURE**: The scatter plot and regression line of the regression $A$ on $T$. </center>

We can see from figure above that the values of $a_n$ and $t_n$ almost fall on a straight line. And the $R^2$ is `r summary(LM)$r.squared`, indicating that the regression fits well.


## Question

Use a Monte Carlo simulation to estimate $\theta=\int_{0}^{1}e^x \text d x$ by the antithetic variate approach while calculate the percent reduction in variance of $\hat{\theta}$ compared with the simple Monte Carlo method theoretically.

## Answer

As $U\sim Uniform(0,1)$, we have

$$
\begin{equation}
\begin{split}
Cov(e^U,e^{1-U}) & =E(e^Ue^{1-U})-E(e^U)E(e^{1-U})\\
& =\int_{0}^{1}e^ue^{1-u} \text d u -\int_{0}^{1}e^u \text d u \int_{0}^{1}e^{1-u} \text d u \\
& = e- e^u|^1_0 \times (-e^{1-u})|^1_0 \\
& =e-(e-1)^2\\
\end{split}
\end{equation}
$$

```{r}

Co <- exp(1)-(exp(1)-1)^2

```


Hence, $Cov(e^U,e^{1-U})=$ `r Co`.

Besides, $Var(e^U)$ and $Var(e^{1-U})$ can be computed as


$$
\begin{equation}
\begin{split}
Var(e^U) & =E(e^{2U})-E^2(e^U)\\
& =\int_{0}^{1}e^{2u} \text d u - (\int_{0}^{1}e^u \text d u)^2 \\
& = \frac{1}{2}e^{2u}|^1_0 - (e^u|^1_0)^2 \\
& =\frac{1}{2}(e^{2}-1)-(e-1)^2\\
\end{split}
\end{equation}
$$


$$
\begin{equation}
\begin{split}
Var(e^{1-U}) & =E(e^{2-2U})-E^2(e^{1-U})\\
& =\int_{0}^{1}e^{2-2u} \text d u - (\int_{0}^{1}e^{1-u} \text d u)^2 \\
& = -\frac{1}{2}e^{2-2u}|^1_0 - (-e^u|^1_0)^2 \\
& =\frac{1}{2}(e^{2}-1)-(1-e)^2\\
\end{split}
\end{equation}
$$


```{r}

VareU <- (1/2)*(exp(2)-1)-(exp(1)-1)^2
Vare1_U <- (1/2)*(exp(2)-1)-(1-exp(1))^2

```

Therefore, $Var(e^U)=Var(e^{1-U})=$ `r VareU`.

Suppose $\hat{\theta}_1$ is the simple MC estimator and $U_1$ and $U_2$ are i.i.d Uniform (0,1). It's easy to know that $(U_1+U_2)/2$ is unbiased for $\theta$ and we have

$$
\begin{equation}
\begin{split}
Var(\frac{e^{U_1}+e^{U_2}}{2}) & = \frac{1}{4} [Var(e^{U_1})+Var(e^{U_2})] \\
& =\frac{1}{4} \times 2\times [\frac{1}{2}(e^2-1)-(e-1)^2]\\
& =\frac{1}{4} (e^2-1)-\frac{1}{2}(e-1)^2\\
\end{split}
\end{equation}
$$

```{r}

Vartherahat1 <- (1/4)*(exp(2)-1)-(1/2)*(exp(1)-1)^2

```

Hence $Var(\hat{\theta}_1)=$ `r Vartherahat1`.

Suppose $\hat{\theta}_2$ is the antithetic estimator and take $U_1=U$, $U_2=1-U$. Then we have


$$
\begin{equation}
\begin{split}
Var(\frac{e^U+e^{1-U}}{2}) & = \frac{1}{4} [Var(e^U)+Var(e^{1-U})+2Cov(e^U,e^{1-U})] \\
& =\frac{1}{4} [2\times(\frac{1}{2}(e^2-1)-(e-1)^2)+2(e-(e-1)^2)]\\
& =\frac{1}{4} (e^2-1)+\frac{1}{2} e-(e-1)^2\\
\end{split}
\end{equation}
$$


```{r}

Vartherahat2 <- (1/4)*(exp(2)-1)+(1/2)*exp(1)-(exp(1)-1)^2

```


Therefore, $Var(\hat{\theta}_2)=$ `r Vartherahat2`.


```{r}

rdu <- (Vartherahat1-Vartherahat2)/Vartherahat1

```

The percent reduction in variance is `r rdu`.


## Question

Use a Monte Carlo simulation to estimate $\theta=\int_{0}^{1}e^x \text d x$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. And compare the result with the theoretical value from Exercise 5.6.

## Answer


the flowchart as follow:

1. Define function $MC.PHi$ to calculate the Monte Carlo simulation estimator of simple Monte Carlo method and antithetic variate approach;
2. Specify m, the number of simulations;
2. estimate $\theta$ by the simple Monte Carlo method and by the antithetic variate approach. Record the results in $MC1$ and $MC2$ respectively;
3.  Compute an empirical estimate of the percent reduction in variance using the antithetic variate.


```{r}

MC.Phi <- function(x, R = 10000, antithetic = FALSE) {
  u <- runif(R/2)
  if (antithetic) v <- 1 - u else v <- runif(R/2)
  u <- c(u, v)
  g <- x*exp(u) # x*exp(u) ~ U(0,x)
  cdf <- mean(g)
  cdf
}


m <- 1000

MC1 <- MC2 <- numeric(m)

x <- 1
for (i in 1:m) {
  MC1[i] <- MC.Phi(x, R = 1000, antithetic = FALSE)
  MC2[i] <- MC.Phi(x, R = 1000, antithetic = TRUE)
}

mean1 <- mean(MC1)
mean2 <- mean(MC2)

var1 <- var(MC1)
var2 <- var(MC2)

rdu2 <- c((var(MC1)-var(MC2))/var(MC1))

di <- abs(rdu-rdu2)

```

In this simulation, the percent reduction in variance is `r rdu2`. They differ by `r di`. We can see that the empirical estimate is very close to the theoretical value from Exercise 5.6.

[Back to the Directory](#directory)













# Homework3 {#question4ans}
## Question

(1) Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are ‘close’ to $g(x) = \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}, \quad x>1$ to help estimate $\int_1^{\infty} g(x) \text{d}x$.  
(2) Pick the importance functions between $f_1$ and $f_2$ that produce the smaller variance by importance sampling, and explain why.

## Answer

Look at the graph of g(x). 

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
x <- seq(1, 10, .01)
g <- x^2*exp(-x^2/2)/(2*pi)^0.5
plot(x,g,type = "l", ylim = c(0, 0.4))
```

From the graph, we might consider a normal distribution or a gamma distribution.

**Part I.** Find a normal distribution and a gamma distribution to be the importance functions respectively.

*Step1:*Supposed that $f_1(x)\sim N(\mu,\tau^2)$, $x>1$. For $g(x)$ we have

$$
\begin{aligned}
&\frac{\text{d}g(x)}{\text{d}x}=\frac{\text{d}}{\text{d}x}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}=\frac{1}{\sqrt{2\pi}}xe^{-x^2/2}(2-x^2) \\
&\frac{\text{d}g(x)}{\text{d}x}=0\Rightarrow x^*=\sqrt{2}
\end{aligned}
$$
g(x) reaches its maximum at $x^*=\sqrt{2}$ in $(1,\infty)$. Therefore, we set $\mu=\sqrt{2}$. As for $\tau$, it should meet that the ratio g(x)/f(x) is nearly constant. We can achieve that by minimizing the variance of $g(x)/f_1(1)$. 


```{r}
x <- seq(1, 10, .01)
g <- x^2*exp(-x^2/2)/(2*pi)^0.5

mu <- (2)^0.5
d1 <- 100000
S1 <- 1000
for (s in seq(0.5, 10, .01)){
  f1 <- dnorm(x, mean = mu, sd = s)
  d <- var(g/f1)
  # try to find f1 that the ratio g(x)/f1(x) is nearly constant.
  if(d < d1){ 
    d1 <- d
    S1 <- s
  }
}
f1 <- dnorm(x, mean = mu, sd = S1)
```

Hence, we set $\tau$=`r S1`, i.e $f_1(x)\sim N($ `r round(mu,4)`,`r round(S1^2,4)`).

*Step2:* Supposed that $f_2(x;\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$, $x>1$. For $f_2(x)$ we have

$$
\begin{aligned}
&\frac{\text{d}f_2(x)}{\text{d}x}=\frac{\text{d}}{\text{d}x}\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-2}e^{-\beta x}[(\alpha-1)-\beta x] \\
&\frac{\text{d}f_2(x)}{\text{d}x}=0\Rightarrow x'=\frac{\alpha-1}{\beta}
\end{aligned}
$$

In order for F and g to have the same peak, we can set $x'=(\alpha-1)/\beta=x^*=\sqrt{2}$. In the same way, minimize the variance of $g(x)/f_1(1)$ to find the most suitable $\alpha$.


```{r}
x <- seq(1, 10, .01)
g <- x^2*exp(-x^2/2)/(2*pi)^0.5

d2 <- 100000
AL2 <- 1000
for (al in seq(1.01, 10, .01)){
  be <- (al-1)/2^0.5
  f2 <- dgamma(x, shape = al, rate = be)
  d <- var(g/f2)
  # try to find f2 that the ratio g(x)/f2(x) is nearly constant.
  if(d < d2){
    d2 <- d
    AL2 <- al
  }
}
BE2 <- (AL2-1)/2^0.5
f2 <- dgamma(x, shape = AL2, rate = BE2)
```

Therefore, we set $\alpha$=`r round(AL2,4)`, $\beta$=`r round(BE2,4)`.

The curves of $g(x)$, $f_1(x)$ and $f_2(x)$ with the support set $(1,\infty)$ are show as follows.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
y <- g
plot(x,y,type = "l", ylim = c(0, 0.7))
lines(x,f1,lty = 6)
lines(x,f2,lty = 2)
legend("topright", inset = 0.02, legend = c("g(x)", "f1","f2"), lty = c(1,6,2))

```

**Part II.** Find the importance function with the smaller variance between $f_1$ and $f_2$.

Compare the ratios $g(x)/f_i(x)$, $i=1,2$.


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
gg <- 10000*rep(1,length(x))
fg1 <- g/f1
fg2 <- g/f2

plot(x,gg,ylim = c(0, 0.8))
lines(x,fg1,lty = 1)
lines(x,fg2,lty = 2)
legend("topright", inset = 0.02, legend = c("fg1","fg2"), lty = c(1,2))
```

From the plot, we can see that the ratio $g(x)/f(x)$ is closer to a constant function. And we also find that $Var(g(x)/f_1(x))=$ `r round(var(fg1),4)`, $Var(g(x)/f_2(x))=$ `r round(var(fg2),4)`. 

**Conclusion.** We may expect the folded normal importance function to produce the smaller variance in estimating the integral. 

## Question

Estimate $\int_{0}^{1} g(x) \text{d} x = \int_0^1 e^{-t} /\left(1+t^2\right) \text{d} t$ with the importance function $f(x) = e^{-t} /\left(1-e^{-1}\right)$ by stratified importance sampling method. And compare it with the result of Example 5.10. 
Besides, you need to modify the subintervals in Exercise 5.15 without losing the original intent. 

## Answer

**Part I.** Compare the estimate of $\int_0^1 e^{-t} /\left(1+t^2\right) \text{d} t$ without stratification and with stratification before modification.
For the stratified importance sampling estimate, we divide the interval $(0,1)$ into five subintervals, $I_{j}=(\frac{j-1}{5},\frac{j}{5})$, $j = 1, 2, ..., 5.$ On each subinterval define $g_{j}(x)=g(x)$ if $x \in I_{j}$ and $g_{j}(x)=0$ otherwise. We now have $k$ parameters to estimate,


$$
\theta_{j}=\int_{a_{j-1}}^{a_{j}} g_{j}(x) \text{d} x, \quad j=1, \ldots, k
$$

and $\theta=\theta_{1}+\cdots+\theta_{k}$. The conditional densities provide the importance functions on each subinterval. That is, on each subinterval $I_{j}$, the conditional density $f_{j}$ of $X$ is defined by

$$
\begin{aligned}
f_{j}(x)&=f_{X \mid I_{j}}\left(x \mid I_{j}\right) \\
&=\frac{f\left(x, a_{j-1} \leq x<a_{j}\right)}{P\left(a_{j-1} \leq x<a_{j}\right)} \\
&=\frac{f(x)}{1 / k}=k f(x), \quad a_{j-1} \leq x<a_{j} .
\end{aligned}
$$


Then, on the $j^{th}$ subinterval it's reasonable to generate the variables from the density

$$
\frac{5e^{-x}}{1-e^{-1}}, \quad \frac{j-1}{5}<x<\frac{j}{5}, \quad j=1,2,...,5
$$

and the estimate $\hat{\theta}$ becomes the sum of the estimates on the $j^{th}$ subinterval, i.e. $\hat{\theta}=\sum_{j=1}^k\hat{\theta}_j$


```{r}

m <- 10000
g <- function(x) {
  exp(-x-log(1+x^2))*(x>0)*(x<1)
}
set.seed(12)
u <- runif(m) 
# f3 with the inverse transform method
x <- - log(1-u*(1-exp(-1))) # F=(1-exp(-x))/(1-exp(-1)), x=-log(1-F*(1-exp(-1)))
fg1 <- g(x)/(exp(-x)/(1-exp(-1)))
theta.hat1 <- mean(fg1) # theta.hat without stratification
se1 <- sd(fg1) # variance without stratification

M <- 10000
k <- 5 
r <- M/k # replicates per stratum
set.seed(12)
T1 <- numeric(k)
vvar1 <- matrix(0, 1, k) 
f1 <- function(x)k*exp(-x)/(1-exp(-1))*(x>0)*(x<1)

for (j in 1:k) {
  xx <- - log(1 - runif(M/k,(j-1)/k,j/k) * (1 - exp(-1)))
  FG1 <- g(xx)/f1(xx)
  T1[j]<-mean(FG1)
  vvar1[j]<-var(FG1)
}
theta.hat2 <- sum(T1) # theta.hat with stratification
se2 <- sqrt(mean(vvar1)) # variance with stratification

```

```{r}
re1 <- rbind(c(theta.hat1,theta.hat2), c(se1,se2))
result1 <- data.frame(re1)
names(result1)<-c("Without stratification","With stratification")
row.names(result1) <- c('theta.hat','se')
knitr::kable (result1, align="c")
```

From the result above, we can see that in one estimation, the importance sampling estimate without stratification is close to the importance sampling estimate with stratification, as ${\left|\hat{\theta}_1-\hat{\theta}_2\right|}=$ `r abs(theta.hat1-theta.hat2)`. But the standard deviation goes from `r se1` to `r se2`.

**Conclusion1.** Stratified importance sampling is A modification to the importance sampling method.

**Part II.** Compare the estimate of $\int_0^1 e^{-t} /\left(1+t^2\right) \text{d} t$ without stratification and with stratification after modification.

In the class, we discuss the stratified sampling method too. Imitate what we do in class, the integration can be written as

$$
\begin{aligned}
\int_0^1 e^{-t} /\left(1+t^2\right) \text{d} t &=\sum_{i=1}^k \int_{(i-1) / k}^{i / k} e^{-t} /\left(1+t^2\right) \text{d} t \\
&=\sum_{i=1}^k \int_{(i-1) / k}^{i / k} {\left(\frac{e^{-t}}{1+t^2} / \frac{e^{-x}}{1-e^{-1}}\right)} \frac{e^{-x}}{1-e^{-1}} \text{d} t \\
&=\sum_{i=1}^k E \frac{e^{-X_i}}{1+X_i^2}
\end{aligned}
$$
where $X_i\sim F,\quad F(x)=\frac{e^{-x}}{1-e^{-1}}$. Hence, we can slightly modify the density into $\frac{e^{-x}}{1-e^{-1}}$.


```{r}
N <- 50 #number of times to repeat the estimation

M <- 10000
set.seed(12)
estg2 <- matrix(0, N, 1) 
#stdg2 <- matrix(0, N, 1) 
g <- function(x) {
  exp(-x-log(1+x^2))*(x>0)*(x<1)
}
for (i in 1:N) {
  u <- runif(M) 
  # f3 with the inverse transform method
  x <- - log(1-u*(1-exp(-1))) # F=(1-exp(-x))/(1-exp(-1)), x=-log(1-F*(1-exp(-1)))
  fg1 <- g(x)/(exp(-x)/(1-exp(-1)))
  estg2[i, 1] <- mean(fg1) 
  #stdg2[i, 1] <- sd(fg1)
}

theta.hat1 <- mean(estg2) # theta.hat without stratification
se1 <- sd(estg2) # variance without stratification

M <- 10000
k <- 5
r <- M/k #replicates per stratum
set.seed(12)
T1 <- numeric(k)
est1 <- matrix(0, N, 1) 
#stdd1 <- matrix(0, N, 1) 
f1 <- function(x)k*exp(-x)/(1-exp(-1))*(x>0)*(x<1)

for (i in 1:N) {
  for (j in 1:k) {
    xx <- - log(1 - runif(M/k,(j-1)/k,j/k) * (1 - exp(-1)))
    FG1 <- g(xx)/f1(xx)
    T1[j]<-mean(FG1)
  }
  est1[i, 1] <- sum(T1) 
  #stdd1[i, 1] <- sd(T1) 
}
theta.hat2 <- mean(est1) # theta.hat with stratification before modification
se2 <- sd(est1) # variance without stratification before modification

set.seed(12)
T2 <- numeric(k)
est2 <- matrix(0, N, 1) 
#stdd2 <- matrix(0, N, 1)
f2 <- function(x)exp(-x)/(1-exp(-1))*(x>0)*(x<1)
for (i in 1:N) {
  for (j in 1:k) {
    xx <- - log(1 - runif(M/k,(j-1)/k,j/k) * (1 - exp(-1)))
    FG2 <- g(xx)/f2(xx)
    T2[j]<-mean(FG2)
  }
  est2[i, 1] <- mean(T2) 
  #stdd2[i, 1] <- sd(T2)
}
theta.hat3 <- mean(est2) # theta.hat with stratification after modification
se3 <- sd(est2) # variance with stratification after modification

```


```{r}
re2 <- rbind(c(theta.hat1,theta.hat2,theta.hat3), c(se1,se2,se3))
result2 <- data.frame(re2)
names(result2)<-c("Without stratification","With stratification before modification","With stratification after modification")
row.names(result2) <- c('theta.hat','se')
knitr::kable (result2, align="c")
```


From the result above, when we repeat the estimation for $N=50$ times, the importance sampling estimate without stratification is very close to the importance sampling estimate with stratification, as $\hat{\theta}_1=\hat{\theta}_2=\hat{\theta}_3=$ `r round(theta.hat1,4)`. And the standard deviation with stratified importance sampling method makes little difference before and after modification.
With stratification, the standard deviation goes down too, as $se_1=$ `r round(se1,7)` < $se_2=se_3$ `r round(se2,7)`.

**Conclusion2.** The importance sampling estimates in three case have very small differences and the standard deviation with stratified importance sampling method is almost equal before and after modification.

[Back to the Directory](#directory)

























# Homework4 {#question5ans}

## Question

Construct a 95% confidence interval for the $\mu$, a parameter from a lognormal distribution and use a Monte Carlo method to obtain an empirical estimate of the confidence level.

## Answer

In a simulation study, we can use separate functions for data generation, data analysis and result reporting. Here we transform $X$ to normal and estimate $\mu$ with the sample mean of the transformed sample and define different functions to complete the simulation. Function $SAmple1$ to generate data, function $Confidenceint$ to construct the 95% confidence interval and  function $REsult1$ to report the result.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
rm(list = ls())
SAmple1 <- function(m,n){
  # n: the number of samples in one simulation
  # m: the number of simulations
  set.seed(126)
  y <- matrix(0,m,n) # Each row is a simulated sample of size n
  for (i in 1:m){
    x <- rlnorm(n)
    y[i,1:n] <- log(x)
  }
  y
}


Confidenceint <- function(Samp){
  # Samp: an m * n matrix, including all the samples of the simulation
  m <- nrow(Samp)
  n <- ncol(Samp)
  CI <- matrix(0,m,2)
  for (i in 1:m){
    y <- Samp[i,1:n]
    yhat <- mean(y)
    se <- sd(y)/sqrt(n)
    low <- yhat + se * qnorm(c(0.025))
    up <- yhat + se * qnorm(c(0.975)) 
    CI[i,1] <- low
    CI[i,2] <- up
  }
  CI
}



REsult1 <- function(confidenceint){
  # confidenceint: an m * 2 matrix, including all the confidence intervals in the simulation
  m <- nrow(confidenceint)
  LOW <- confidenceint[1:m,1]
  UP <- confidenceint[1:m,2]
  res <- mean(LOW < 0 & UP > 0)
  res
}

set.seed(126)

m <- 10000
n <- 100
sam <- SAmple1(m,n)
ci <- Confidenceint(sam)
re <- REsult1(ci) # an empirical estimate of the confidence level

# output the result
Cii <- data.frame(ci)
names(Cii) <- c('Low bound','Upper bound')
knitr::kable (head(Cii,6),align="c")
re

```

The table above shows the results for partial 95% confidence intervals. And an empirical estimate of the confidence level is `r re`, which is very close to 0.95.

## Question
Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level $\hat{\alpha}\dot{=}0.055$. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)

## Answer

Similarly, we can use separate functions for data generation, data analysis and result reporting. Here we define function $SAmple2$ to generate data, function $COunt5test$ for Count Five test, function $ANalysis2$ to compare the power of the Count Five test and F test, and function $REsult2$ to report the result.
Use Monte Carlo methods to estimate the power, where the sampled distributions are $N(\mu_1=0, \sigma_1^2 = 1^2), N(\mu_2=0,  \sigma_2^2 = 1.5^2)$. And repeat the simulation for $m=10000$ times for $n=20,30$ as small sample sizes, for for $n=60,90$ as medium sample sizes, and for $n=200,500$ as large sample sizes.

```{r}

rm(list = ls())

SAmple2 <- function(sigma1,sigma2,m,n){
  set.seed(12)
  Samp1 <- matrix(0,m,n)
  Samp2 <- matrix(0,m,n)
  for (i in 1:m){
    x <- rnorm(n, 0, sigma1)
    y <- rnorm(n, 0, sigma2)
    Samp1[i,1:n] <- x
    Samp2[i,1:n] <- y
  }
  SAmp <- rbind(Samp1,Samp2)
  SAmp
}


COunt5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  cou5te <- as.integer(max(c(outx, outy)) > 5)
  cou5te
}

ANalysis2 <- function(Samp) {
  m <- nrow(Samp)/2
  n <- ncol(Samp)
  Test <- matrix(0,2,m)
  for (i in 1:m){
    x <- Samp[i,1:n]
    y <- Samp[m+i,1:n]
    C5 <- COunt5test(x, y)
    Fp <- var.test(x, y)$p.value
    Ftest <- as.integer(Fp <= 0.055)
    Test[1:2,i] <- c(C5, Ftest)
  }
  Test
}

REsult2 <- function(Test) {
  RE <- c(n,rowMeans(Test))
  RE
}

sigma1 <- 1
sigma2 <- 1.5
m <- 10000
N <- c(20, 30, 60, 90, 200, 500)
K <- length(N)
REsu <- matrix(0,K,3)
for (k in 1:K){
  n <- N[k]
  samp <- SAmple2(sigma1,sigma2,m,n)
  test <- ANalysis2(samp)
  REsu[k,1:3] <- REsult2(test)
}

resu <- data.frame(REsu)
names(resu) <- c('n','Power of COunt5test','Power of Ftest')
knitr::kable (resu,align="c")

```
The table above shows  the power of the Count Five test and F test for small, medium, and large sample sizes with $n$ in [20, 30, 60, 90, 200, 500]. The simulation results suggest that the power of two tests increases with the size of the sample increasing, and the F test for equal variance is more powerful in this case, for all sample sizes compared.

## Question
Use suitable method to check whether the results of the powers for two methods under a particular simulation setting are consistent and try to answer the following questions or meet the requirements:

$\quad$ • What is the corresponding hypothesis test problem?

$\quad$ • Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

$\quad$ • Please provide the least necessary information for hypothesis


## Answer
We can't say the powers are different at 0.05 level. We should use hypothesis testing to determine this.

• The corresponding hypothesis test problem is whether the two powers are different, that is: $H_0: power_1=power_2$ vs $H_1: power_1 \ne power_2$.

• We can use paired-t test.

• There is only one sample for each of $power_1$ and $power_2$ in the title, so multiple experiments should be performed to generate a plurality of $power_1$ and $power_2$, thereby obtaining a plurality of differences of $power_1$ and $power_2$. Define the difference between $power_1$ and $power_2$ as $d$. As the simulation setting was derived from 10,000 experiments, it is noted that a plurality of $d$ approximately obey the t distribution. Therefore, the null hypothesis can be tested by applying paired-t test.


[Back to the Directory](#directory)

























# Homework5 {#question6ans}

## Question
Refer to the air-conditioning data set **aircondit** provided in the boot package. The 12 observations are the times in hours between failures of air-conditioning equipment [63, Example 1.1]:
$$3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.$$
Assume that the times between failures follow an exponential model Exp($\lambda$). Obtain the MLE of the hazard rate $\lambda$ and use bootstrap to estimate the bias and standard error of the estimate.

## Answer
The **aircondit** data is a data frame with 1 variable, so aircondit[1] extracts the 12 observations. 
As the times between failures follow an exponential model Exp(λ), the Log-likelihood function is
$$ln[l(X|\lambda)]=-\lambda\sum_{i=1}^nx_i+nln(\lambda)$$
therefore we have

$$
\begin{aligned}
& \frac{\partial ln[l(X|\lambda)]}{\partial \lambda}=-\sum_{i=1}^nx_i+\frac{n}{\lambda} \\
& \frac{\partial ln[l(X|\lambda)]}{\partial \lambda}=0 \\
& \Rightarrow -\sum_{i=1}^nx_i+\frac{n}{\lambda}=0\\
& \Rightarrow \hat{\lambda}=\frac{1}{\bar{x}}
\end{aligned}
$$

Hence the MLE of $\lambda$ is $1/\bar{x}$. 
We can use **boot** package to print the estimates of bias and standard error. As what we discussed in class, we can use separate functions for data generation, data analysis and result reporting. In this exercise, define function **Sample1** to extract data, function **Rate1** to construct the statistic and function **Result1** to report the result.
 
```{r}
rm(list = ls())

library(boot)

Sample1 <- function(x){
  samp <- aircondit[x]
  samp
}

Rate1 <- function(samp, i) {
  rat <- 1/mean(as.matrix(samp[i, ]))
  rat
}

Result1 <- function(samp,func,Rr){
  bo <- boot(samp, statistic = func, R = Rr)
  print(bo)
}

set.seed(1234)
samp <- Sample1(1)
resu <- Result1(samp,Rate1,2000)

detach(package:boot)

rm(list = ls())
```

From the result above, we can see that the the bias of the estimate is 0.001369 and standard error of the estimate is 0.004986. 



## Question
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer 
Similarly, use **aircondit[1]** to extract the data. In this exercise, define function **Sample2** to extract data, function **Meant2** to construct the statistic and function **Result2** to report the result.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
rm(list = ls())

library(boot)

Sample2 <- function(x){
  samp <- aircondit[x]
  samp
}

Meant2 <- function(x, i) {
  mea <- mean(as.matrix(x[i, ]))
  mea
}

Result2 <- function(samp,func,Rr){
  bo <- boot(samp, statistic = func, R = Rr)
  re <- boot.ci(bo, type = c("norm", "perc", "basic", "bca"))
  print(bo)
  print(re)
  hist(bo$t, prob = TRUE, main = " ")
  points(bo$t0, 0, cex = 2, pch = 16)
  bo
}

set.seed(1234)
samp <- Sample2(1)
resu <- Result2(samp,Meant2,2000)

detach(package:boot)

rm(list = ls())

```


The 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal method is (33.0, 182.8), by the basic method is (19.9, 170.0), by the percentile method is (46.2, 196.2) and by BCa method is (58.5, 232.2). They differ from each other and the length of intervals are -149.8, -150.1, -150 and -173.5 respectively.

The replicates are not approximately normal, so the normal and percentile intervals differ. From the histogram of replicates, it appears that the distribution of the replicates is skewed, although we are estimating a mean. The reason is that the sample size is too small for CLT to give a good approximation here. The BCa interval is a percentile type interval, but it adjusts for both skewness and bias, so the BCa interval differs from the others.


## Question
Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left, and the proportion of times that the confidence intervals miss on the right.

## Answer

```{r}
rm(list = ls())

skewness <- function(x,i) {
  #computes the sample skewness coeff.
  x_bar <- mean(x[i])
  x_bar
}

Sample3 <- function(n, mea, sd){
  samp <- rnorm(n, mea, sd)
  samp
}

Analysis3 <- function(m, func, Rr, n, mea, sd){
  library(boot)
  nornorm <- matrix(0, m, 2)
  norbasi <- matrix(0, m, 2)
  norperc <- matrix(0, m, 2)
  for (i in 1:m) {
    Samp <- Sample3(n, mea, sd)
    Skew <- boot(Samp, statistic = func, R=Rr)
    Nor <- boot.ci(Skew, type=c("norm","basic","perc"))
    nornorm[i,] <- Nor$norm[2:3]
    norbasi[i,] <- Nor$basic[4:5]
    norperc[i,] <- Nor$percent[4:5]
  }
  #Calculate the coverage probability of a normal distribution
  norm <- mean(nornorm[,1] <= s & nornorm[,2] >= s)
  basi <- mean(norbasi[,1] <= s & norbasi[,2] >= s)
  perc <- mean(norperc[,1] <= s & norperc[,2] >= s)
  #Calculate the probability of the left side of the normal distribution
  normleft <- mean(nornorm[,1] >= s )
  basileft <- mean(norbasi[,1] >= s )
  percleft <- mean(norperc[,1] >= s )
  #Calculate the right side probability of a normal distribution
  normright <- mean(nornorm[,2] <= s )
  basiright <- mean(norbasi[,2] <= s )
  percright <- mean(norperc[,2] <= s )
  analyresu <- c(norm, basi, perc, normleft, basileft, percleft, normright, basiright, percright)
  analyresu
}

Result3 <- function(sd, analyresu){
  dnam <- paste("N ( 0 ,", as.character(sd^2),")",seq="")
  Distribution <- c(dnam)
  Type <- c("basic", "norm", "perc")
  Left <- analyresu[4:6]
  Right <- analyresu[7:9]
  P.coverage <- analyresu[1:3]
  result <- data.frame(Distribution, Type, Left, Right, P.coverage)
  result
}

s <- 0
n <- 20
m <- 1000
R <- 1000

mea <- 0
sd <- 3 

# We can set n, m, R, mea, sd any way we want.

set.seed(1234)
library(boot)

Analyresu <- Analysis3(m, skewness, R, n, mea, sd)
Resu <- Result3(sd, Analyresu)

knitr::kable (Resu, align="c")

rm(list = ls())
```

From the result above, we can find that in the case of the size of sample is 20, the coverage probabilities of the 3 types of confidence intervals (the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval.) are very close to 0.9 in the normal distribution (skewness 0). And we can get the probability that the confidence intervals miss on the left, and the probability that the confidence intervals miss on the right from the table. The two probability are both small, and the results of three methods are very close.

[Back to the Directory](#directory)









# Homework6 {#question7ans}

## Question
Refer to Exercise 7.7. Efron and Tibshirani discuss the following example [84, Ch. 7]. The five-dimensional scores data have a $5 \times 5$ covariance matrix $\Sigma$, with positive eigenvalues $\lambda_1>\cdots>\lambda_5$. In principal components analysis,
$$
\theta=\frac{\lambda_1}{\sum_{j=1}^5 \lambda_j}
$$
measures the proportion of variance explained by the first principal component. Let $\hat{\lambda}_1>\cdots>\hat{\lambda}_5$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. Compute the sample estimate
$$
\hat{\theta}=\frac{\hat{\lambda}_1}{\sum_{j=1}^5 \hat{\lambda}_j}
$$
of $\theta$. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.


## Answer

We can use package $bootstrap$ to Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

```{r}
rm(list = ls())

set.seed(1234)

library(bootstrap)
attach(scor)

Sample1 <- function(scor){
  x <- as.matrix(scor)
  return(x)
}

Analysis1 <- function(x){
  n <- nrow(x)
  theta_jack <- numeric(n)
  lambda <- eigen(cov(x))$values
  theta_hat <- max(lambda/sum(lambda))
  for (i in 1:n) {
    y <- x[-i, ]
    s <- cov(y)
    lambda <- eigen(s)$values
    theta_jack[i] <- max(lambda/sum(lambda))
  }
  bias_jack <- (n - 1) * (mean(theta_jack) - theta_hat)
  se_jack <- sqrt((n - 1)/n * sum((theta_jack - mean(theta_jack))^2)) 
  
  re <- c(theta_hat, bias_jack, se_jack)
  
  return(re)
}

Result1 <- function(theta_hat, bias_jack, se_jack){
  Re1 <- data.frame(theta_hat, bias_jack, se_jack)
  names(Re1) <- c('est','bias','se')
  return(Re1)
}

x <- Sample1(scor)
anRE <- Analysis1(x)
Re1 <- Result1(anRE[1],anRE[2],anRE[3])

knitr::kable (Re1,align="c")

set.seed(1234)

attach(scor)

th <- function(x, i) {
  y <- as.matrix(x[i, ])
  s <- cov(y)
  e <- eigen(s)
  lambda <- e$values
  max(lambda/sum(lambda))
}

Result1_2 <- function(scor){
  library(boot)
  boot(scor, statistic = th, R = 2000)
}

Re2 <- Result1_2(scor)
Re2

rm(list = ls())
```

As the result above, the jackknife estimate of bias of $\hat{\theta}$ is 0.0010 and the jackknife estimate of se is 0.0495. The bootstrap estimates of bias of $\hat{\theta}$ is 0.0015 and the bootstrap estimate of se is 0.0479. These estimates of two methods are very close.



## Question
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer

We can compare the five models in Example 7.18 or Example 7.17 by the leave-two-out cross validation, which can be accomplished by a nested for loop.


```{r}
rm(list = ls())

set.seed(1234)

library(DAAG, warn.conflict = FALSE)
attach(ironslag)

Analysis2 <- function(magnetic){
  n <- length(magnetic)
  N <- choose(n, 2)
  e1 <- numeric(N)
  e2 <- numeric(N)
  e3 <- numeric(N)
  e4 <- numeric(N)
  e5 <- numeric(N)
  ij <- 1
  for (i in 1:(n - 1)) for (j in (i + 1):n) {
    k <- c(i, j)
    y <- magnetic[-k]
    x <- chemical[-k]
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
    e1[ij] <- sum((magnetic[k] - yhat1)^2)
    J2 <- lm(y ~ x + I(x^2))
    
    yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
      J2$coef[3] * chemical[k]^2
    e2[ij] <- sum((magnetic[k] - yhat2)^2)
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
    yhat3 <- exp(logyhat3)
    e3[ij] <- sum((magnetic[k] - yhat3)^2)
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
    yhat4 <- exp(logyhat4)
    e4[ij] <- sum((magnetic[k] - yhat4)^2)
    c2 <- x^2
    c3 <- x^3
    J5 <- lm(y ~ x + c2 + c3)
    yhat5 <- J5$coef[1] + J5$coef[2] * chemical[k] +
      J5$coef[3] * chemical[k]^2 + J5$coef[4] * chemical[k]^3
    e5[ij] <- sum((magnetic[k] - yhat5)^2)
    ij <- ij + 1
  }
  Re <- c(sum(e1), sum(e2), sum(e3), sum(e4), sum(e5))/N
  return(Re)
}

Re <- Analysis2(magnetic)
Re

rm(list = ls())

```

By leave-two-out cross-validation, we can see from the result above that the quadratic model, i.e. model (2) achieve the minimum prediction error. Hence, the quadratic model is selected by leave-two-out cross-validation.


## Question

Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

## Answer

Implement the bivariate Spearman rank correlation test for independence as a permutation test. Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples. We take bivariate normal samples as an example to compare the two p-values. 

```{r}
rm(list = ls())

set.seed(433)

spear.perm <- function(x, y) {
  stest <- cor.test(x, y, method = "spearman")
  n <- length(x)
  rs <- replicate(R, expr = {
    k <- sample(1:n)
    cor.test(x, y[k], method = "spearman")$estimate
  })
  rs1 <- c(stest$estimate, rs)
  pval <- mean(as.integer(stest$estimate <= rs1))
  return(list(rho.s = stest$estimate, p.value = pval))
}

set.seed(4321)

library(MASS)
mu <- c(0, 0)
Sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
n <- 30
R <- 499

Sample3 <-function(n, mu, Sigma){
  x <- mvrnorm(n, mu, Sigma)
  return(x)
}
x <- Sample3(n, mu, Sigma)
cor.test(x[, 1], x[, 2], method = "spearman")
spear.perm(x[, 1], x[, 2])


rm(list = ls())
```

We expect that the p-values for cor.test and spear.perm should be approximately equal. As the result above, p-values for cor.test and spear.perm are 0.002061 and 0.002. The p-values are both significant and close in value as we expected.
[Back to the Directory](#directory)





























# Homework7 {#question8ans}

## Question

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

## Answer
The standard Laplace density is

$$
f(x)=\frac{1}{2} e^{-|x|}
$$

and

$$
r\left(x_t, y\right)=\frac{f(y)}{f\left(x_t\right)}=\frac{e^{-|y|}}{e^{-\left|x_t\right|}}=e^{\left|x_t\right|-|y|} .
$$

To generate the standard Laplace distribution, we write a function named **Laplacerw**. And the input $N$ is the length of the chain to generate, $x0$ is the initial value and $sigma$ is the standard deviation of the normal proposal distribution. At each step, the candidate point is generated from $N\left(\mu_t, \sigma^2\right)$, where $\mu_t=X_t$ is the previous value in the chain. Finally, the function output the chain and the number of rejected points.

We take different variances from (0.5, 1, 3, 5) to generate different random walk chains. Set the length of the chain $N=5500$ and generate the initial value $x0$ from Standard normal distribution in this exercise.

```{r}
rm(list = ls())

set.seed(123451)

Laplacerw <- function(N, x0, sigma) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    xt <- x[i - 1]
    y <- rnorm(1, xt, sigma)
    if (u[i] <= exp(abs(xt) - abs(y)))
      x[i] <- y
    else {
      x[i] <- x[i - 1]
      k <- k + 1
    }
  }
  return(list(x = x, k = k))
}

N <- 5500
sigma <- c(0.5, 1, 2, 5)
x0 <- rnorm(1)
rw1 <- Laplacerw(N, x0, sigma[1])
rw2 <- Laplacerw(N, x0, sigma[2])
rw3 <- Laplacerw(N, x0, sigma[3])
rw4 <- Laplacerw(N, x0, sigma[4])

rejectionrates <- c(rw1$k, rw2$k, rw3$k, rw4$k)/N
acceptancerates <- rep(1,4)-rejectionrates

Rates <- data.frame(rbind(rejectionrates, acceptancerates))
names(Rates)<-c('Chain1','Chain2','Chain3','Chain4')
row.names(Rates) <- c('Rejection rates','Acceptance rates')
knitr::kable (Rates, align="c")
```

The rejection rate should not be too high or too low, and a rejection rate of 20% ~ 50% is appropriate. As the result above, maybe the third chain is the best one. 


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
b <- 100
y1 <- rw1$x[(b + 1):N]
y2 <- rw2$x[(b + 1):N]
y3 <- rw3$x[(b + 1):N]
y4 <- rw4$x[(b + 1):N]


#par(mfrow = c(2, 2))
plot(rw1$x, type = "l")
plot(rw2$x, type = "l")
plot(rw3$x, type = "l")
plot(rw4$x, type = "l")
#par(mfrow = c(1, 1))


#par(mfrow = c(2, 2))
p <- ppoints(200)
y <- qexp(p, 1)
z <- c(-rev(y), y)
k <- 4
fx <- 0.5 * exp(-abs(z))
hist(y1, breaks = "Scott", freq = FALSE, ylim = c(0,0.5))
lines(z, fx)
hist(y2, breaks = "Scott", freq = FALSE, ylim = c(0,0.5))
lines(z, fx)
hist(y3, breaks = "Scott", freq = FALSE, ylim = c(0,0.5))
lines(z, fx)
hist(y4, breaks = "Scott", freq = FALSE, ylim = c(0,0.5))
lines(z, fx)
#par(mfrow = c(1, 1))


#par(mfrow = c(2, 2))
Q1 <- quantile(y1, p)
qqplot(z, Q1, cex = 0.4)
abline(0, 1)
Q2 <- quantile(y2, p)
qqplot(z, Q2, cex = 0.4)
abline(0, 1)
Q3 <- quantile(y3, p)
qqplot(z, Q3, cex = 0.4)
abline(0, 1)
Q4 <- quantile(y4, p)
qqplot(z, Q4, cex = 0.4)
abline(0, 1)
#par(mfrow = c(1, 1))
```

Based on the plots above, the size of  burn-in sample is $b=100$. And each of the chains seems to have converged to the target Laplace distribution. The third chain corresponding to $\sigma = 2$ may have the best fit based on the QQ plots and is more effective than others.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}

allchain <- t(as.matrix(data.frame(y1,y2,y3,y4)))

Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + B/n+(B/(n*k))     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}

#compute diagnostic statistics
psi <- t(apply(allchain, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
r_hat <- Gelman.Rubin(psi)


#plot psi for the four chains
#par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi[i,1:ncol(psi)], type="l",
       xlab=i, ylab=bquote(psi))
#par(mfrow=c(1,1)) #restore default

#plot the sequence of R-hat statistics
rhat <- rep(0, ncol(psi))
for (j in 1:ncol(psi))
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[1:ncol(psi)], type="l", xlab="", ylab="R",ylim=c(1,3.5))
abline(h=1.2, lty=2)

rm(list = ls())
```
We also plot the cumulative mean plot of four chains and the trend of $\hat{R}$ to monitor convergence of the chain by Gelman-Rubin method. In the same way, each of the chains seems to have converged to the target Laplace distribution. After about 2000 iterations, the cumulative means tend to be stable. At this time, $\hat{R}$ is about 1.1, less than 1.2. Hence the chains smoothly converge to the target distribution.

## Question
Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y = \beta_0 + \beta_1X$ to the sample and check the residuals of the model for normality and constant variance.

## Answer 
We use Gibbs sampler to generate random numbers from $N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2)=N(0,0,1,1)$. It's easy to know that  $X_1 \mid X_2$ and $X_2 \mid X_1$ are still normally distributed, and we have

$$
f\left(x_1 \mid x_2\right) \sim N\left(\mu_1+\rho \frac{\sigma_1}{\sigma_2}\left(x_2-\mu_2\right),\left(1-\rho^2\right) \sigma_1^2\right)\\
f\left(x_2 \mid x_1\right) \sim N\left(\mu_2+\rho \frac{\sigma_2}{\sigma_1}\left(x_1-\mu_1\right),\left(1-\rho^2\right) \sigma_2^2\right)
$$
We take different point from $\left\{(0,0), (1,1), (-1,-1), (2,2) \right\}$ as initial point to generate different random walk chains. Set the length of the chain $N=5000$ and generate the initial value $x0$ from Standard normal distribution in this exercise.
At each step, the candidate point is generated from Normal distribution and update $X_t$ and $Y_t$ over time. Finally, the function output the chains of $X$ and $Y$.
The Gibbs algorithm is as follows.

```{r}
#rm(list = ls())

set.seed(12340)

N <- 5000
burn <- 1000
rho <- 0.9

k <- 4
MU1 <- c(0,1,-1,2)
MU2 <- c(0,1,-1,2)

Mu1chain <- matrix(0,k,(N-burn))
Mu2chain <- matrix(0,k,(N-burn))

for (kkk in (1:4)){
  X <- matrix(0, N, 2)
  mu1 <- 0
  mu2 <- 0
  sigma1 <- 1
  sigma2 <- 1
  s1 <- sqrt(1 - rho^2) * sigma1
  s2 <- sqrt(1 - rho^2) * sigma2
  X[1, ] <- c(MU1[kkk], MU2[kkk])
  for (i in 2:N) {
    x2 <- X[i - 1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    X[i, 1] <- rnorm(1, m1, s1)
    x1 <- X[i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    X[i, 2] <- rnorm(1, m2, s2)
  }
  b <- burn + 1
  x <- X[b:N, ]
  Mu1chain[kkk, ] <- x[, 1]
  Mu2chain[kkk, ] <- x[, 2]
}

k <- 4
Linearmodel <- matrix(0,k,2)

for (kk in (1:k)){
  L <- lm(Mu2chain[kk,] ~ Mu1chain[kk,])
  Linearmodel[kk,] <- L$coefficients
}

Linearpa <- data.frame(Linearmodel)
names(Linearpa)<-c("beta0","beta1")
row.names(Linearpa) <- c('chain1','chain2','chain3','chain4')
knitr::kable (Linearpa, align="c")
```

The parameters of linear model of each chain are shown in the table above. We can see that $\beta_1$ are all close to 0.9, which indicates that the coefficients of the fitted model match the parameters of the target distribution well. 

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}

#par(mfrow=c(2,2))
for (kk in (1:k)){
  plot(Mu1chain[kk,], Mu2chain[kk,], cex = 0.25,xlab = 'x',ylab = 'y')
  abline(h = 0, v = 0)
}
#par(mfrow=c(1,1)) #restore default


#par(mfrow=c(2,2))
for (kk in (1:k)){
  L <- lm(Mu2chain[kk,] ~ Mu1chain[kk,])
  plot(L$fit, L$res, cex = 0.25)
  abline(h = 0)
}
#par(mfrow=c(1,1)) #restore default


#par(mfrow=c(2,2))
for (kk in (1:k)){
  L <- lm(Mu2chain[kk,] ~ Mu1chain[kk,])
  qqnorm(L$res, cex = 0.25)
  qqline(L$res)
}
#par(mfrow=c(1,1)) #restore default

Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + B/n+(B/(n*k))     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}
```

The scatterplots of the generated chains are shown below. The samples have the elliptical symmetry and location at the origin of the target bivariate normal distribution. The strong positive correlation is also evident in the plots.
The plots of residuals vs fits suggest that the error variance is constant with respect to the response variable. The QQ plots of residuals are consistent with the normal error assumption of the linear model.


```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
allchain1 <- as.matrix(data.frame(Mu1chain))


#compute diagnostic statistics
psi1 <- t(apply(allchain1, 1, cumsum))
for (i in 1:nrow(psi1))
  psi1[i,] <- psi1[i,] / (1:ncol(psi1))
r_hat1 <- Gelman.Rubin(psi1)

allchain2 <- as.matrix(data.frame(Mu2chain))


#compute diagnostic statistics
psi2 <- t(apply(allchain2, 1, cumsum))
for (i in 1:nrow(psi2))
  psi2[i,] <- psi2[i,] / (1:ncol(psi2))
r_hat2 <- Gelman.Rubin(psi2)

k <- 4


#plot psi for the four chains
#par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi1[i,1:ncol(psi1)], type="l",xlab=i, ylab=bquote(psi1))
#par(mfrow=c(1,1)) #restore default


#par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi2[i,1:ncol(psi2)], type="l",xlab=i, ylab=bquote(psi2))
#par(mfrow=c(1,1)) #restore default


#plot the sequence of R-hat statistics
#par(mfrow=c(1,2))
rhat1 <- rep(0, ncol(psi1))
for (j in 1:ncol(psi1))
  rhat1[j] <- Gelman.Rubin(psi1[,1:j])
plot(rhat1[1:ncol(psi1)], type="l", xlab="", ylab="R",ylim=c(1,3.5))
abline(h=1.2, lty=2)

rhat2 <- rep(0, ncol(psi2))
for (j in 1:ncol(psi2))
  rhat2[j] <- Gelman.Rubin(psi2[,1:j])
plot(rhat2[1:ncol(psi2)], type="l", xlab="", ylab="R",ylim=c(1,3.5))
abline(h=1.2, lty=2)
#par(mfrow=c(1,1))

rm(list = ls())
```

We also plot the cumulative mean plot of four chains of $X$ and $Y$, and plot the trend of $\hat{R}$ to monitor convergence of the chains by Gelman-Rubin method. Each of the chains seems to have converged to the target distribution. After about 2000 iterations, the cumulative means tend to be stable. At this time, both two $\hat{R}$ are about 1.1, less than 1.2. Hence the chains smoothly converge to the target distribution.
[Back to the Directory](#directory)


























# Homework8 {#question9ans}
## Question

Consider the a mediation effect model. Test the mediation effect 
$$H_0:\alpha \beta=0 \quad vs \quad H_1:\alpha \beta \neq 0$$
with the statistics
$$
T = \frac{\hat{\alpha}\hat{\beta}}{\hat{se}(\hat{\alpha}\hat{\beta})}
$$
When we want to use the permutation test, there are three ways to set the parameters:

$$
\begin{aligned}
&1) \quad \alpha=0,\quad \beta=1 \quad  (X \perp \!\!\! \perp M)\\
&2) \quad \alpha=1,\quad \beta=0 \quad  (M \perp \!\!\! \perp Y)\\
&3) \quad \alpha=0,\quad \beta=0 \quad  (X \perp \!\!\! \perp M, M \perp \!\!\! \perp Y)
\end{aligned}
$$

Please indicate whether these three parameter settings can control probability of type I error of $H_0$.

Specifically, consider the model

$$
\begin{aligned}
&M=a_M+\alpha X+e_M \\
&Y=a_Y+\beta M+ \gamma X +e_Y \\
&e_M,e_Y \stackrel{i.i.d}{\sim} N(0,1)
\end{aligned}
$$
and set up a simulation study to examine the performance of the above three permutation tests. 

## Answer 

$a_M,$ $a_Y$ and $\gamma$ are constants, and we set $a_M=a_Y=\gamma=1$. We specify that $X \sim N(0,1)$.


```{r,include=FALSE}
rm(list = ls())
library(bda)
```

```{r}
# Set parameters
N <- 5000 # size of sample
Pr <- 2000 # number of replicates
aM <- 1
aY <- 1

################ Case1: alpha = 0; beta = 1 ################

##### 1.1 Data generation
alpha <- 0
beta <- 1
gamma <- 1
set.seed(12345)
eM <- rnorm(N,0,1)
eY <- rnorm(N,0,1)
X <- rnorm(N,0,1)
M <- aM*rep(1,N) + alpha*X + eM
Y <- aY*rep(1,N) + beta*M + gamma*X + eY

##### 1.2 Data analysis
# Obtain statistics and p-value
T0 <- mediation.test(M,X,Y)$Sobel[1]
P0_1 <- mediation.test(M,X,Y)$Sobel[2]
# Permutation test
T <- matrix(0,1,Pr)
for (k in 1:Pr){
  i_i <-  sample(1:N, N, replace = FALSE)
  PX <- matrix(0,1,N)
  for (i in 1:N){
    PX[i] <- X[i_i[i]]
  }
  PX <- as.numeric(PX)
  t <- mediation.test(M,PX,Y)$Sobel[1]
  T[k] <- t
}
# p-value by permutation test
P1_1 <- sum(abs(T)>abs(T0))/Pr

################ Case2: alpha = 1; beta = 0 ################

##### 2.1 Data generation
alpha <- 1
beta <- 0
gamma <- 1
set.seed(1242)
eM <- rnorm(N,0,1)
eY <- rnorm(N,0,1)
X <- rnorm(N,0,1)
M <- aM*rep(1,N) + alpha*X + eM
Y <- aY*rep(1,N) + beta*M + gamma*X + eY

##### 2.2 Data analysis
# Obtain statistics and p-value
T0 <- mediation.test(M,X,Y)$Sobel[1]
P0_2 <- mediation.test(M,X,Y)$Sobel[2]
# Permutation test
T <- matrix(0,1,Pr)
for (k in 1:Pr){
  i_i <-  sample(1:N, N, replace = FALSE)
  PY <- matrix(0,1,N)
  for (i in 1:N){
    PY[i] <- Y[i_i[i]]
  }
  PY <- as.numeric(PY)
  t <- mediation.test(M,X,PY)$Sobel[1]
  T[k] <- t
}
# p-value by permutation test
P1_2 <- sum(abs(T)>abs(T0))/Pr

################ Case3: alpha = 0; beta = 0 ################

##### 3.1 Data generation
alpha <- 0
beta <- 0
gamma <- 1
set.seed(12345)
eM <- rnorm(N,0,1)
eY <- rnorm(N,0,1)
X <- rnorm(N,0,1)
M <- aM*rep(1,N) + alpha*X + eM
Y <- aY*rep(1,N) + beta*M + gamma*X + eY

##### 3.2 Data analysis
# Obtain statistics and p-value
T0 <- mediation.test(M,X,Y)$Sobel[1]
P0_3 <- mediation.test(M,X,Y)$Sobel[2]
# Permutation test
T <- matrix(0,1,Pr)
for (k in 1:Pr){
  i_i <-  sample(1:N, N, replace = FALSE)
  PX <- matrix(0,1,N)
  for (i in 1:N){
    PX[i] <- X[i_i[i]]
  }
  PX <- as.numeric(PX)
  j_j <-  sample(1:N, N, replace = FALSE)
  PY <- matrix(0,1,N)
  for (j in 1:N){
    PY[j] <- Y[j_j[j]]
  }
  PY <- as.numeric(PY)
  t <- mediation.test(M,PX,PY)$Sobel[1]
  T[k] <- t
}
# p-value by permutation test
P1_3 <- sum(abs(T)>abs(T0))/Pr


##### 3 Result reporting
P0 <- c(P0_1,P0_2,P0_3)
P1 <- c(P1_1,P1_2,P1_3)
result <- data.frame(t(cbind(P0,P1)))
names(result)<-c("alpha=0,beta=1","alpha=1,beta=0","alpha=0,beta=0")
row.names(result) <- c('p-value wiht H0','p-value with permutation test')
knitr::kable (result, align="c")

rm(list = ls())
```

As the table above, we can see that in three cases, the p-values with permutation test are smaller than the p-values with H0, which indicates that none of the three cases can control probability of type I error of $H_0$. 

## Question

Consider the model 
$$
\begin{aligned}
&P(Y=1|X_1,X_2,X_3)=expit(\alpha+b_1X_1+b_2X_2+b_3X_3) \\
&X_1 \sim P(1), \quad X_2 \sim Exp(1), \quad X_3 \sim B(1,0.5)
\end{aligned}
$$

(1) Write an R function implementing the functions above with an input value of $N,b_1,b_2,b_3,f_0$ and an output value of $\alpha$.

(2) Call this function with an input value of $N=10^6,b_1=0,b_2=1,b_3=-1,f_0=0.1,0.01,0.001,0.0001$.

(3) Draw the scatter plot of $f_0$ vs. $\alpha$.

## Answer

(1) Define the function **ALPHA** to implement the functions mentioned.

```{r}
rm(list = ls())
ALPHA <- function(N,b1,b2,b3,f0) {
  alpha <- matrix(0,1:length(f0))
  for (i in 1:length(f0)){
    f_0 <- f0[i]
    g <- function(alp){
      tmp <- exp(-alp-b1*x1-b2*x2-b3*x3)
      p <- 1/(1+tmp)
      mean(p) - f_0
    }
    solution <- uniroot(g,c(-20,20))
    alpha[i] <- solution$root
  }
  alpha
}
```

(2) Call the function with the input value mentioned.
```{r}
##### Set parameters
N <- 1e6;
b1 <- 0; 
b2 <- 1; 
b3 <- -1
f0 <- c(0.1,0.01,0.001,0.0001)

##### Data generation
set.seed(12345)
x1 <- rpois(N, 1)
x2 <- rexp(N, 1)
x3 <- rbinom(N,1,0.5)

##### Data analysis
alpha <- ALPHA(N,b1,b2,b3,f0)
```

(3) Draw the scatter plot of $f_0$ vs. $\alpha$.

```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
##### Result reporting
plot(-log(f0),alpha)
rm(list = ls())
```
As we can see from the plot above,$-log(f_0)$ and $\alpha$ are almost negatively correlated as expected.
[Back to the Directory](#directory)


























# Homework9 {#question10ans}
## Question

Define that $X_1, \cdots , X_n \sim \text{Exp}(\lambda)$. For some reason, we just know that $X_i$ falls in an interval $(u_i,v_i)$, where $u_i$ and $v_i$ are two non-random known constants, and $u_i<v_i$. This type of data is called interval deleted data.

(1) Try to directly maximize the likelihood function of the observed data and use EM algorithm to solve the MLE of $\lambda$, and prove that the two are equal.

(2) Let the observation data of $(u_i,v_i)$, $i=1, \cdots n$ $(n=10)$ be $(11,12),$ $(8,9),$ $(27,28),$ $(13,14),$ $(16,17),$ $(0,1),$ $(23,24),$ $(10,11),$ $(24,25),$ $(2,3)$. Try to program the numerical solutions of MLE of $\lambda$ of the above two algorithms respectively.

Remark: The likelihood function of the observed data is $L(\lambda)=\prod_{i=1}^n P_{\lambda} (u_i \le X_i \le v_i)$.

## Answer 
$(1)$ The the observed data likelihood function is

$$
\begin{aligned}
L_o(\lambda) & = \prod_{i=1}^n P_{\lambda} (u_i \le X_i \le v_i) \\
& = \prod_{i=1}^n \int_{u_i}^{v_i} \lambda e^{-\lambda x} \text{d} x \\
& = \prod_{i=1}^n(e^{-\lambda u_i}-e^{-\lambda v_i})
\end{aligned}
$$ 
and the observed data log-likelihood function is $lnL_o(\lambda) = \sum_{i=1}^{n} ln (e^{-\lambda u_i}-e^{-\lambda v_i})$

***Method 1: Obtain the MLE of $\lambda$ by maximizing the observed data log-likelihood function ***


$$
\begin{aligned}
& \frac{\partial lnL_o(\lambda)}{\partial \lambda} =  0\\
& \Rightarrow \sum_{i=1}^{n} \frac{v_ie^{-\hat{\lambda} v_i}-u_ie^{-\hat{\lambda} u_i}}{e^{-\hat{\lambda} v_i}-e^{-\hat{\lambda} u_i}} =0
\end{aligned}
$$
We can get a score function by the expression above that $S(\lambda) \equiv \sum_{i=1}^{n} \frac{v_ie^{-\hat{\lambda} v_i}-u_ie^{-\hat{\lambda} u_i}}{e^{-\hat{\lambda} v_i}-e^{-\hat{\lambda} u_i}}$ and the MLE of $\lambda$ satisfies that $S(\hat{\lambda})=0$.


***Method 2: Obtain the MLE of $\lambda$ by EM algorithm ***

Let's supplement the value of $X_i$, $i=1,2, \cdots,n$. The complete data likelihood function is 
$$
L_c(\lambda)=\prod_{i=1}^n \lambda e^{-\lambda X_i}=\lambda^n e^{\sum_{i=1}^n X_i}
$$
and the complete data log-likelihood function is
$$
ln L_c(\lambda)= n ln\lambda - \lambda \sum_{i=1}^n X_i
$$
$\quad$ **E step** Calculate the expectation and solve for $Q(\lambda, \hat{\lambda}^{(t)})$
Define $I={1,2, \cdots,n }$, then we have

$$
\begin{aligned}
Q(\lambda, \hat{\lambda}^{(t)}) & = E \left[ n ln\lambda - \lambda \sum_{i=1}^n X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (u_i,v_i), i \in I  \right] \\
& =  n ln\lambda - \lambda \sum_{i=1}^n E \left[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (u_i,v_i), i \in I \right]
\end{aligned}
$$
and

$$
\begin{aligned}
E \left[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (u_i,v_i), i \in I \right]& = [ E[X_i]-E[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (0,u_i), i \in I]P(0< X_i <u_i) \\
&-E[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (v_i,\infty), i \in I]P(v_i< X_i <\infty) ]  \bigg/  P(u_i<X_i<v_i)
\end{aligned}
$$
where we have

$$
\begin{aligned}
& E[X_i]=\frac{1}{\hat{\lambda}^{(t)}} \\
& E[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (0,u_i), i \in I] = \frac{E[X_i]-E[X_i \mid X_i, X_i \in (u_i,\infty), i \in I]P(u_i< X_i <\infty)}{P(0< X_i <u_i)} \\ & =\frac{\frac{1}{\hat{\lambda}^{(t)}}-(\frac{1}{\hat{\lambda}^{(t)}}+u_i)e^{-\hat{\lambda}^{(t)} u_i}}{1-e^{-\hat{\lambda}^{(t)} u_i}}=\frac{1}{\hat{\lambda}^{(t)}}-\frac{u_i e^{-\hat{\lambda}^{(t)} u_i}}{1-e^{-\hat{\lambda}^{(t)} u_i}} \\
& E[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (v_i,\infty), i \in I] = \frac{1}{\hat{\lambda}^{(t)}}+v_i \\
& P(0< X_i <u_i) =  \int_{0}^{u_i} \hat{\lambda}^{(t)} e^{-\hat{\lambda}^{(t)}\lambda x} \text{d} x = 1-e^{-\hat{\lambda}^{(t)} u_i} \\
& P(u_i< X_i <v_i) =  \int_{u_i}^{v_i} \hat{\lambda}^{(t)} e^{-\hat{\lambda}^{(t)} x} \text{d} x = e^{-\hat{\lambda}^{(t)} u_i} - e^{-\hat{\lambda}^{(t)} v_i}\\
& P(v_i< X_i <\infty) = \int_{v_i}^{\infty} \hat{\lambda}^{(t)} e^{-\hat{\lambda}^{(t)} x} \text{d} x = e^{-\hat{\lambda}^{(t)} v_i}
\end{aligned}
$$

hence, we have 

$$
\begin{aligned}
E \left[X_i \mid \hat{\lambda}^{(t)}, X_i, X_i \in (u_i,v_i), i \in I \right] &= \frac{\frac{1}{\lambda}- \left(\frac{1}{\lambda}-\frac{u_i e^{-\lambda u_i}}{1-e^{-\lambda u_i}} \right) \left(1-e^{-\lambda u_i} \right)- \left( \frac{1}{\lambda}+v_i \right) e^{-\lambda v_i}}{e^{-\lambda u_i} - e^{-\lambda v_i}} \\
& = \frac{1}{\lambda} + \frac{u_i e^{-\lambda u_i} - v_i e^{-\lambda v_i}}{e^{-\lambda u_i} - e^{-\lambda v_i}}
\end{aligned}
$$
Substitute the expectation into $Q(\lambda, \hat{\lambda}^{(t)})$, we have

$$
Q(\lambda, \hat{\lambda}^{(t)}) =  n ln\lambda - \lambda \sum_{i=1}^n  \left( \frac{1}{\hat{\lambda}^{(t)}} + \frac{u_i e^{-\hat{\lambda}^{(t)} u_i} - v_i e^{-\hat{\lambda}^{(t)} v_i}}{e^{-\hat{\lambda}^{(t)} u_i} - e^{-\hat{\lambda}^{(t)} v_i}} \right)
$$


$\quad$ **M step** Maximize $Q(\lambda, \hat{\lambda}^{(t)})$ on $\lambda$ to get $\hat{\lambda}^{(t+1)}$

$$
\begin{aligned}
& \frac{\partial Q(\lambda, \hat{\lambda}^{(t)})}{\partial \lambda} = 0 \\
& \Rightarrow \hat{\lambda}^{(t+1)} = n \bigg/ \sum_{i=1}^n  \left( \frac{1}{\hat{\lambda}^{(t)}} + \frac{u_i e^{-\hat{\lambda}^{(t)} u_i} - v_i e^{-\hat{\lambda}^{(t)} v_i}}{e^{-\hat{\lambda}^{(t)} u_i} - e^{-\hat{\lambda}^{(t)} v_i}} \right) 
\end{aligned}
$$

Under some mild conditions, $\hat{\lambda}^{(t)}$ converges to the MLE for the observed data (Wu, 1983). Hence, we have the equation

$$
\begin{aligned}
& \hat{\lambda} = n \bigg/ \sum_{i=1}^n  \left( \frac{1}{\hat{\lambda}} + \frac{u_i e^{-\hat{\lambda} u_i} - v_i e^{-\hat{\lambda} v_i}}{e^{-\hat{\lambda} u_i} - e^{-\hat{\lambda} v_i}} \right) \\
& \Rightarrow \sum_{i=1}^{n} \frac{v_ie^{-\hat{\lambda} v_i}-u_ie^{-\hat{\lambda} u_i}}{e^{-\hat{\lambda} v_i}-e^{-\hat{\lambda} u_i}} =0
\end{aligned} 
$$

Therefore, the two estimators are equal.


$(2)$ Plot the curve of $S(\lambda)$ to determine the interval where the null point is



```{r,fig.align ='center',fig.height=3.5, fig.width=5.5}
U <- c(11,8,27,13,16,0,23,10,24,2)
V <- c(12,9,28,14,17,1,24,11,25,3)

# Function with equation
fun <- function(x) {-(U[1]*exp(-x*U[1])-V[1]*exp(-x*V[1]))/(exp(-x*U[1])-exp(-x*V[1])) - (U[2]*exp(-x*U[2])-V[2]*exp(-x*V[2]))/(exp(-x*U[2])-exp(-x*V[2])) - (U[3]*exp(-x*U[3])-V[3]*exp(-x*V[3]))/(exp(-x*U[3])-exp(-x*V[3])) - (U[4]*exp(-x*U[4])-V[4]*exp(-x*V[4]))/(exp(-x*U[4])-exp(-x*V[4])) - (U[5]*exp(-x*U[5])-V[5]*exp(-x*V[5]))/(exp(-x*U[5])-exp(-x*V[5])) - (U[6]*exp(-x*U[6])-V[6]*exp(-x*V[6]))/(exp(-x*U[6])-exp(-x*V[6])) - (U[7]*exp(-x*U[7])-V[7]*exp(-x*V[7]))/(exp(-x*U[7])-exp(-x*V[7])) - (U[8]*exp(-x*U[8])-V[8]*exp(-x*V[8]))/(exp(-x*U[8])-exp(-x*V[8])) - (U[9]*exp(-x*U[9])-V[9]*exp(-x*V[9]))/(exp(-x*U[9])-exp(-x*V[9])) - (U[10]*exp(-x*U[10])-V[10]*exp(-x*V[10]))/(exp(-x*U[10])-exp(-x*V[10]))}
  

curve(fun, from = 0, to = 1); abline(h = 0, lty = 3)
```

As the plot above, the null point is in the interval (0,0.2).

```{r}
mle1 <- uniroot(fun, lower = 0, upper = 0.2)
mle1
```

Hence, the MLE of $\lambda$ obtained by maximizing the observed data log-likelihood function is `r mle1$root`.

the EM algorithm is as follows

```{r}
rm(list = ls())
U <- c(11,8,27,13,16,0,23,10,24,2)
V <- c(12,9,28,14,17,1,24,11,25,3)

EM <- function(U,V,max.it=10000,eps=1e-8){
  n <- length(U)
  i <- 1
  theta1 <- 0.01
  theta2 <- 0.02
  while( abs(theta1 - theta2) >= eps){
    theta1 <- theta2
    fun <- function(x) {(U[1]*exp(-x*U[1])-V[1]*exp(-x*V[1]))/(exp(-x*U[1])-exp(-x*V[1])) + (U[2]*exp(-x*U[2])-V[2]*exp(-x*V[2]))/(exp(-x*U[2])-exp(-x*V[2])) + (U[3]*exp(-x*U[3])-V[3]*exp(-x*V[3]))/(exp(-x*U[3])-exp(-x*V[3])) + (U[4]*exp(-x*U[4])-V[4]*exp(-x*V[4]))/(exp(-x*U[4])-exp(-x*V[4])) + (U[5]*exp(-x*U[5])-V[5]*exp(-x*V[5]))/(exp(-x*U[5])-exp(-x*V[5])) + (U[6]*exp(-x*U[6])-V[6]*exp(-x*V[6]))/(exp(-x*U[6])-exp(-x*V[6])) + (U[7]*exp(-x*U[7])-V[7]*exp(-x*V[7]))/(exp(-x*U[7])-exp(-x*V[7])) + (U[8]*exp(-x*U[8])-V[8]*exp(-x*V[8]))/(exp(-x*U[8])-exp(-x*V[8])) + (U[9]*exp(-x*U[9])-V[9]*exp(-x*V[9]))/(exp(-x*U[9])-exp(-x*V[9])) + (U[10]*exp(-x*U[10])-V[10]*exp(-x*V[10]))/(exp(-x*U[10])-exp(-x*V[10]))}
    su <- fun(theta1)
    theta2 <- n/(n/theta1 + su)
    print(round(c(theta2),9))
    if(i == max.it) break
    i <- i + 1    
  }
  return(theta2)
}

mle2 <- EM(U,V,max.it=10000,eps=1e-5)


```

Hence, the MLE of $\lambda$ obtained by EM algorithm  is `r mle2`.
The two estimates obtained by two method are are theoretically equivalent and the two values are numerically close.

## Question
Why do you need to use *unlist()* to convert a list to an atomic vector? Why doesn’t *as.vector()* work?

## Answer
To get rid of (flatten) the nested structure.

## Question
Why is *1 == "1"* true? Why is *-1 < FALSE* true? Why is *"one" < 2* false?

## Answer 
These operators are all functions which coerce their arguments (in these cases) to character, double and character. To enlighten the latter case: “one” comes after “2” in ASCII.

[Back to the Question](#question)


## Question
What does *dim()* return when applied to a vector?

## Answer
*NULL*

## Question
If *is.matrix(x)* is TRUE, what will *is.array(x)* return?

## Answer
*TRUE*, as also documented in *?array*:

A two-dimensional array is the same thing as a matrix.

## Question
What attributes does a data frame possess?

## Answer
names, row.names and class.

## Question
What does *as.matrix()* do when applied to a data frame with columns of different types?

## Answer
From *?as.matrix*:

The method for data frames will return a character matrix if there is only atomic columns and any non-(numeric/logical/complex) column, applying as.vector to factors and format to other non-character columns. Otherwise the usual coercion hierarchy (logical < integer < double < complex) will be used, e.g., all-logical data frames will be coerced to a logical matrix, mixed logical-integer will give a integer matrix, etc.


## Question
Can you have a data frame with 0 rows? What about 0 columns?

## Answer 
Yes, we can create them easily. Also both dimensions can be 0:

```{r}
iris[FALSE,]

iris[ , FALSE] # or iris[FALSE]

iris[FALSE, FALSE] # or just data.frame()

```


[Back to the Directory](#directory)



























# Homework10 {#question11ans}

## Question

The function below scales a vector so it falls in the range [0,1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame?

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```

## Answer 
Since this function needs numeric input, one can check this via an if clause. If one also wants to return non-numeric input columns, these can be supplied to the else argument of the function "else if()". Take freeny as an example.

```{r}
data.frame(lapply(freeny, function(x) if (is.numeric(x)) scale01(x) else x))
```

## Question 
Use vapply() to:
a) Compute the standard deviation of every column in a numeric data frame.
b) Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply()
twice.)

## Answer 

a) As a numeric data.frame we choose attitude:
```{r}
vapply(attitude, sd, numeric(1))
```

b) And as a mixed data.frame we choose freeny:

```{r}
vapply(freeny[vapply(freeny, is.numeric, logical(1))], sd, numeric(1))

```

## Question
Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard deviations, and correlation 0.9.

• Write an Rcpp function.

• Compare the corresponding generated random numbers with pure R language using the function “qqplot”.

• Compare the computation time of the two functions with the function “microbenchmark”.


## Answer 

• The Rcpp function **gibbsC**:

```{r,eval=FALSE}
library(Rcpp)
#// This is the rw_MetropolisC.cpp
#include <Rcpp.h>
#using namespace Rcpp;
#// [[Rcpp::export]]
cppFunction('NumericMatrix gibbsC(int N, double mu1, double mu2, double sigma1, double sigma2, double pho) {
  NumericMatrix mat(N, 2);
  double s1 = sqrt(1-pho * pho) * sigma1, s2 = sqrt(1-pho * pho) * sigma2;
  double x = 0, y = 0;
  for(int i = 0; i < N; i++) {
    double m1 = mu1 + pho * (y - mu2) * sigma1 / sigma2;
    x = rnorm(1, m1, s1)[0];
    mat(i, 0) = x;

    double m2 = mu2 + pho * (x - mu1) * sigma2 / sigma1;
    y = rnorm(1, m2, s2)[0];
    mat(i, 1) = y;
  }
  return(mat);
}')
```


• We generate random numbers with pure R language and Rcpp and the QQ plots are as follows. 

```{r,eval=FALSE}
library(Rcpp)
library(microbenchmark)
set.seed(123)

gibbsR <- function(N, mu1, mu2, sigma1, sigma2, pho) {
  mat <- matrix(nrow = N, ncol = 2)
  x <- 0
  y <- 0
  s1 = sqrt(1-pho * pho) * sigma1
  s2 = sqrt(1-pho * pho) * sigma2
  for (i in 1:N) {
    m1 = mu1 + pho * (y - mu2) * sigma1 / sigma2;
    x = rnorm(1, m1, s1);
    mat[i, 1] = x;
    
    m2 = mu2 + pho * (x - mu1) * sigma2 / sigma1;
    y = rnorm(1, m2, s2);
    mat[i, 2] = y;
  }
  mat
}

gibbR=gibbsR(2000,0,0,1,1,0.9)
gibbC=gibbsC(2000,0,0,1,1,0.9)

par(mfrow = c(1, 3))
qqplot(gibbC,gibbR)
qqplot(gibbC[,1],gibbR[,1])
qqplot(gibbC[,2],gibbR[,2])
```

There are three graphs here, which are the QQ plot of two-dimensional random numbers and the two components respectively. We can see that the points in the three graphs are all clustered on the diagonal, which indicates that the two sets of random numbers come from almost the same distribution.

• Use the function “microbenchmark” to compare the computation time of the two functions.

```{r,eval=FALSE}

ts <- microbenchmark(gibbR=gibbsR(2000,0,0,1,1,0.9), gibbC=gibbsC(2000,0,0,1,1,0.9))
knitr::kable(summary(ts)[,c(1,3,5,6)])

```

We can see that the speed of Rcpp functions is much faster than that of pure R language function.


[Back to the Directory](#directory)


















